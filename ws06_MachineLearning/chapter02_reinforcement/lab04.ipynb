{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\", render_mode = \"human\")\n",
    "\n",
    "print(env.observation_space.n)\n",
    "print(env.action_space.n)\n",
    "\n",
    "\n",
    "for _ in range(1):\n",
    "    observation = env.reset()[0]\n",
    "    \n",
    "    for _ in range(10):\n",
    "        env.render() #GUI 출력\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        observation, reward, done, _, info = env.step(action)\n",
    "        print(\"action\", action, \"reward\", reward)\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa9cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 100\n",
      "Episode: 200\n",
      "Episode: 300\n",
      "Episode: 400\n",
      "Episode: 500\n",
      "Episode: 600\n",
      "Episode: 700\n",
      "Episode: 800\n",
      "Episode: 900\n",
      "Episode: 1000\n",
      "Episode: 1100\n",
      "Episode: 1200\n",
      "Episode: 1300\n",
      "Episode: 1400\n",
      "Episode: 1500\n",
      "Episode: 1600\n",
      "Episode: 1700\n",
      "Episode: 1800\n",
      "Episode: 1900\n",
      "Episode: 2000\n",
      "Episode: 2100\n",
      "Episode: 2200\n",
      "Episode: 2300\n",
      "Episode: 2400\n",
      "Episode: 2500\n",
      "Episode: 2600\n",
      "Episode: 2700\n",
      "Episode: 2800\n",
      "Episode: 2900\n",
      "Episode: 3000\n",
      "Episode: 3100\n",
      "Episode: 3200\n",
      "Episode: 3300\n",
      "Episode: 3400\n",
      "Episode: 3500\n",
      "Episode: 3600\n",
      "Episode: 3700\n",
      "Episode: 3800\n",
      "Episode: 3900\n",
      "Episode: 4000\n",
      "Episode: 4100\n",
      "Episode: 4200\n",
      "Episode: 4300\n",
      "Episode: 4400\n",
      "Episode: 4500\n",
      "Episode: 4600\n",
      "Episode: 4700\n",
      "Episode: 4800\n",
      "Episode: 4900\n",
      "Episode: 5000\n",
      "Episode: 5100\n",
      "Episode: 5200\n",
      "Episode: 5300\n",
      "Episode: 5400\n",
      "Episode: 5500\n",
      "Episode: 5600\n",
      "Episode: 5700\n",
      "Episode: 5800\n",
      "Episode: 5900\n",
      "Episode: 6000\n",
      "Episode: 6100\n",
      "Episode: 6200\n",
      "Episode: 6300\n",
      "Episode: 6400\n",
      "Episode: 6500\n",
      "Episode: 6600\n",
      "Episode: 6700\n",
      "Episode: 6800\n",
      "Episode: 6900\n",
      "Episode: 7000\n",
      "Episode: 7100\n",
      "Episode: 7200\n",
      "Episode: 7300\n",
      "Episode: 7400\n",
      "Episode: 7500\n",
      "Episode: 7600\n",
      "Episode: 7700\n",
      "Episode: 7800\n",
      "Episode: 7900\n",
      "Episode: 8000\n",
      "Episode: 8100\n",
      "Episode: 8200\n",
      "Episode: 8300\n",
      "Episode: 8400\n",
      "Episode: 8500\n",
      "Episode: 8600\n",
      "Episode: 8700\n",
      "Episode: 8800\n",
      "Episode: 8900\n",
      "Episode: 9000\n",
      "Episode: 9100\n",
      "Episode: 9200\n",
      "Episode: 9300\n",
      "Episode: 9400\n",
      "Episode: 9500\n",
      "Episode: 9600\n",
      "Episode: 9700\n",
      "Episode: 9800\n",
      "Episode: 9900\n",
      "Episode: 10000\n",
      "Episode: 10100\n",
      "Episode: 10200\n",
      "Episode: 10300\n",
      "Episode: 10400\n",
      "Episode: 10500\n",
      "Episode: 10600\n",
      "Episode: 10700\n",
      "Episode: 10800\n",
      "Episode: 10900\n",
      "Episode: 11000\n",
      "Episode: 11100\n",
      "Episode: 11200\n",
      "Episode: 11300\n",
      "Episode: 11400\n",
      "Episode: 11500\n",
      "Episode: 11600\n",
      "Episode: 11700\n",
      "Episode: 11800\n",
      "Episode: 11900\n",
      "Episode: 12000\n",
      "Episode: 12100\n",
      "Episode: 12200\n",
      "Episode: 12300\n",
      "Episode: 12400\n",
      "Episode: 12500\n",
      "Episode: 12600\n",
      "Episode: 12700\n",
      "Episode: 12800\n",
      "Episode: 12900\n",
      "Episode: 13000\n",
      "Episode: 13100\n",
      "Episode: 13200\n",
      "Episode: 13300\n",
      "Episode: 13400\n",
      "Episode: 13500\n",
      "Episode: 13600\n",
      "Episode: 13700\n",
      "Episode: 13800\n",
      "Episode: 13900\n",
      "Episode: 14000\n",
      "Episode: 14100\n",
      "Episode: 14200\n",
      "Episode: 14300\n",
      "Episode: 14400\n",
      "Episode: 14500\n",
      "Episode: 14600\n",
      "Episode: 14700\n",
      "Episode: 14800\n",
      "Episode: 14900\n",
      "Episode: 15000\n",
      "Episode: 15100\n",
      "Episode: 15200\n",
      "Episode: 15300\n",
      "Episode: 15400\n",
      "Episode: 15500\n",
      "Episode: 15600\n",
      "Episode: 15700\n",
      "Episode: 15800\n",
      "Episode: 15900\n",
      "Episode: 16000\n",
      "Episode: 16100\n",
      "Episode: 16200\n",
      "Episode: 16300\n",
      "Episode: 16400\n",
      "Episode: 16500\n",
      "Episode: 16600\n",
      "Episode: 16700\n",
      "Episode: 16800\n",
      "Episode: 16900\n",
      "Episode: 17000\n",
      "Episode: 17100\n",
      "Episode: 17200\n",
      "Episode: 17300\n",
      "Episode: 17400\n",
      "Episode: 17500\n",
      "Episode: 17600\n",
      "Episode: 17700\n",
      "Episode: 17800\n",
      "Episode: 17900\n",
      "Episode: 18000\n",
      "Episode: 18100\n",
      "Episode: 18200\n",
      "Episode: 18300\n",
      "Episode: 18400\n",
      "Episode: 18500\n",
      "Episode: 18600\n",
      "Episode: 18700\n",
      "Episode: 18800\n",
      "Episode: 18900\n",
      "Episode: 19000\n",
      "Episode: 19100\n",
      "Episode: 19200\n",
      "Episode: 19300\n",
      "Episode: 19400\n",
      "Episode: 19500\n",
      "Episode: 19600\n",
      "Episode: 19700\n",
      "Episode: 19800\n",
      "Episode: 19900\n",
      "Training finished.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 82\u001b[0m     action \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39margmax(q_table[state])\n\u001b[0;32m     83\u001b[0m     state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import random\n",
    "from os import system, name\n",
    "from time import sleep\n",
    "\n",
    "# Define function to clear console window.\n",
    "def clear(): \n",
    "  \n",
    "    # Clear on Windows.\n",
    "    if name == 'nt': \n",
    "        _ = system('cls')\n",
    "  \n",
    "    # Clear on Mac and Linux. (os.name is 'posix') \n",
    "    else: \n",
    "        _ = system('clear')\n",
    "\n",
    "clear()\n",
    "\n",
    "\"\"\"Setup\"\"\"\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env # Setup the Gym Environment\n",
    "\n",
    "# Make a new matrix filled with zeros.\n",
    "# The matrix will be 500x6 as there are 500 states and 6 actions.\n",
    "q_table = numpy.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "training_episodes = 20000 # Amount of times to run environment while training.\n",
    "display_episodes = 10 # Amount of times to run environment after training.\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1 # Learning Rate\n",
    "gamma = 0.6 # Discount Rate\n",
    "epsilon = 0.1 # Chance of selecting a random action instead of maximising reward.\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "\"\"\"Training the Agent\"\"\"\n",
    "\n",
    "for i in range(training_episodes):    \n",
    "    state = env.reset()[0] # Reset returns observation state and other info. We only need the state.\n",
    "    done = False\n",
    "    penalties, reward, = 0, 0\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Pick a new action for this state.\n",
    "        else:\n",
    "            action = numpy.argmax(q_table[state]) # Pick the action which has previously given the highest reward.\n",
    "\n",
    "        next_state, reward, done, _, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action] # Retrieve old value from the q-table.\n",
    "        next_max = numpy.max(q_table[next_state])\n",
    "\n",
    "        # Update q-value for current state.\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10: # Checks if agent attempted to do an illegal action.\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "    if i % 100 == 0: # Output number of completed episodes every 100 episodes.\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "\n",
    "\"\"\"Display and evaluate agent's performance after Q-learning.\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "\n",
    "for _ in range(display_episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0   \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = numpy.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        clear()\n",
    "        env.render()\n",
    "        print(f\"Timestep: {epochs}\")\n",
    "        print(f\"State: {state}\")\n",
    "        print(f\"Action: {action}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "        sleep(0.15) # Sleep so the user can see the \n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {display_episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / display_episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / display_episodes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
