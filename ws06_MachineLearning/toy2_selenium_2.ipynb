{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주제 : 뉴스를 입력받아 야구/ E-sports/ 그외 뉴스인지 분류\n",
    "\n",
    "# E-sports 뉴스 크롤링\n",
    "    \n",
    "    \n",
    "# 1. 데이터 수집, 전처리\n",
    "# 2. 모델 생성 및 학습\n",
    "# 3. 평가, 예측프로그램 작성\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "454dda33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "League_of_Legends의 1번째 기사 크롤\n",
      "League_of_Legends의 2번째 기사 크롤\n",
      "League_of_Legends의 3번째 기사 크롤\n",
      "League_of_Legends의 4번째 기사 크롤\n",
      "League_of_Legends의 5번째 기사 크롤\n",
      "League_of_Legends의 6번째 기사 크롤\n",
      "League_of_Legends의 7번째 기사 크롤\n",
      "League_of_Legends의 8번째 기사 크롤\n",
      "League_of_Legends의 9번째 기사 크롤\n",
      "League_of_Legends의 10번째 기사 크롤\n",
      "League_of_Legends의 11번째 기사 크롤\n",
      "League_of_Legends의 12번째 기사 크롤\n",
      "League_of_Legends의 13번째 기사 크롤\n",
      "League_of_Legends의 14번째 기사 크롤\n",
      "League_of_Legends의 15번째 기사 크롤\n",
      "League_of_Legends의 16번째 기사 크롤\n",
      "League_of_Legends의 17번째 기사 크롤\n",
      "League_of_Legends의 18번째 기사 크롤\n",
      "League_of_Legends의 19번째 기사 크롤\n",
      "League_of_Legends의 20번째 기사 크롤\n",
      "Valorant의 1번째 기사 크롤\n",
      "general의 1번째 기사 크롤\n",
      "general의 2번째 기사 크롤\n",
      "general의 3번째 기사 크롤\n",
      "general의 4번째 기사 크롤\n",
      "general의 5번째 기사 크롤\n",
      "general의 6번째 기사 크롤\n",
      "general의 7번째 기사 크롤\n",
      "general의 8번째 기사 크롤\n",
      "general의 9번째 기사 크롤\n",
      "general의 10번째 기사 크롤\n",
      "general의 11번째 기사 크롤\n",
      "general의 12번째 기사 크롤\n",
      "general의 13번째 기사 크롤\n",
      "general의 14번째 기사 크롤\n",
      "general의 15번째 기사 크롤\n",
      "general의 16번째 기사 크롤\n",
      "general의 17번째 기사 크롤\n",
      "general의 18번째 기사 크롤\n",
      "general의 19번째 기사 크롤\n",
      "general의 20번째 기사 크롤\n",
      "저장완료\n"
     ]
    }
   ],
   "source": [
    "# 기본코드\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "game_list = [\"League_of_Legends\", \"Player_Unknowns_Battle_Grounds\", \"Valorant\", \"general\"]\n",
    "game_list_2 = [\"lol\", \"pubg\", \"valorant\", \"esports_general\"]\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for idx in range(len(game_list)):\n",
    "#for idx in range(1):\n",
    "    driver.get(f\"https://game.naver.com/esports/{game_list[idx]}/news/{game_list_2[idx]}\") \n",
    " \n",
    "    elements = driver.find_elements(By.XPATH, '//*[@id=\"civ\"]/div/div/div/div[2]/div[1]/div[2]/div[1]/ul//a')\n",
    "    links = [element.get_attribute('href') for element in elements]\n",
    "    \n",
    "    data = {\"Title\":[],\"Content\": []}\n",
    "\n",
    "    #for in_idx,link_url, in enumerate(links ,start=1):\n",
    "    i = 0\n",
    "    for link_url in links:\n",
    "        \n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(link_url)  \n",
    "        sleep(2)\n",
    "        \n",
    "\n",
    "        html = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        target_elements = soup.find(\"div\", class_=\"news_end font1 size3\")\n",
    "\n",
    "        text_only = target_elements.get_text()\n",
    "        title_text_only = soup.find(\"h4\", class_=\"title\")\n",
    "        \n",
    "        data[\"Title\"].append(title_text_only.get_text())\n",
    "        data[\"Content\"].append(text_only)\n",
    "        i = i + 1\n",
    "        print(f'{game_list[idx]}의 {i}번째 기사 크롤')\n",
    "    \n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('e-sports.csv', encoding='utf-8')\n",
    "print('저장완료')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f90400d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general의 1번째 기사 크롤\n",
      "general의 2번째 기사 크롤\n",
      "general의 3번째 기사 크롤\n",
      "general의 4번째 기사 크롤\n",
      "general의 5번째 기사 크롤\n",
      "general의 6번째 기사 크롤\n",
      "general의 7번째 기사 크롤\n",
      "general의 8번째 기사 크롤\n",
      "general의 9번째 기사 크롤\n",
      "general의 10번째 기사 크롤\n",
      "general의 11번째 기사 크롤\n",
      "general의 12번째 기사 크롤\n",
      "general의 13번째 기사 크롤\n",
      "general의 14번째 기사 크롤\n",
      "general의 15번째 기사 크롤\n",
      "general의 16번째 기사 크롤\n",
      "general의 17번째 기사 크롤\n",
      "general의 18번째 기사 크롤\n",
      "general의 19번째 기사 크롤\n",
      "저장완료\n"
     ]
    }
   ],
   "source": [
    "# 24.03.04 날짜 기사 수가 부족해서, 기사가 많이 올라온 날 링크 따로 잡아 크롤\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "game_list = [\"League_of_Legends\", \"Player_Unknowns_Battle_Grounds\", \"Valorant\", \"general\"]\n",
    "game_list_2 = [\"lol\", \"pubg\", \"valorant\", \"esports_general\"]\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://game.naver.com/esports/League_of_Legends/news/lol?date=2024-03-01\") \n",
    " \n",
    "elements = driver.find_elements(By.XPATH, '//*[@id=\"civ\"]/div/div/div/div[2]/div[1]/div[2]/div[1]/ul//a')\n",
    "links = [element.get_attribute('href') for element in elements]\n",
    "    \n",
    "data = {\"Title\":[],\"Content\": []}\n",
    "\n",
    "\n",
    "i = 0\n",
    "for link_url in links:\n",
    "        \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(link_url)  \n",
    "    sleep(2)\n",
    "        \n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    target_elements = soup.find(\"div\", class_=\"news_end font1 size3\")\n",
    "\n",
    "    text_only = target_elements.get_text()\n",
    "    title_text_only = soup.find(\"h4\", class_=\"title\")\n",
    "        \n",
    "    data[\"Title\"].append(title_text_only.get_text())\n",
    "    data[\"Content\"].append(text_only)\n",
    "    i = i + 1\n",
    "    print(f'{game_list[idx]}의 {i}번째 기사 크롤')\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('lol.csv', encoding='utf-8')\n",
    "print('저장완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f05b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엑셀 파일 합치기 완료\n"
     ]
    }
   ],
   "source": [
    "# 03.04 기사와 03.01기사 파일 병합\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 합칠 엑셀 파일들이 있는 디렉토리 경로\n",
    "directory_path = \"C:\\workspace\\ws06_MachineLearning\"\n",
    "\n",
    "# 해당 디렉토리에서 모든 엑셀 파일의 경로 가져오기\n",
    "excel_files = glob.glob(f\"{directory_path}/*.xlsx\")\n",
    "\n",
    "# 모든 엑셀 파일을 담을 빈 DataFrame 생성\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# 각 엑셀 파일을 읽어와서 DataFrame에 추가\n",
    "for excel_file in excel_files:\n",
    "    df = pd.read_excel(excel_file)\n",
    "    merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "\n",
    "# 결과를 새로운 엑셀 파일로 저장\n",
    "merged_data.to_excel(\"C:\\workspace\\ws06_MachineLearning/merged_data.xlsx\", index=False)\n",
    "\n",
    "print(\"엑셀 파일 합치기 완료\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
