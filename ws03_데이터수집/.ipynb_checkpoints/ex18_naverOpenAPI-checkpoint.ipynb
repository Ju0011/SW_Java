{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0mkkdlKwzf-D",
    "outputId": "84d3e000-8b82-43b6-9b2b-6d0eb6fc3059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네이버 검색 프로그램입니다.\n",
      ">검색할 키워드를 입력하세요.\n",
      ">검색 카테고리를 선택하세요 \n",
      " 1. 네이버 뉴스,                    2. 네이버 블로그 3. 네이버 카페\n",
      "******* \n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL : https://openapi.naver.com/v1/search/news.json?query=\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 100\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 저장 완료\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m :\n\u001b[1;32m--> 100\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[5], line 59\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_naver_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_all.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (search_text, sNode)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m---> 59\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mwrite(result)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(filename \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 저장 완료\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#2. 크롤링 결과 중 일부만 파싱할 때\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not None"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "client_id = \"5BE6Vc9HVCIU7_SKWbY_\"\n",
    "client_secret = \"GQe2e_vSC2\"\n",
    "\n",
    "def makeURL(sNode, search_text ):\n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = \"/%s.json\" % sNode\n",
    "    parameters = \"?query=%s\" % urllib.parse.quote(search_text)\n",
    "    url = base + node + parameters\n",
    "    return url\n",
    "\n",
    "\n",
    "def requestURL(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"X-Naver-Client-Id\",   client_id)  # open api 키를 header에 추가\n",
    "    req.add_header(\"X-Naver-Client-Secret\",  client_secret)  # open api 키를 header에 추가\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)  # 크롤링\n",
    "\n",
    "        if response.status == 200:  # 200이면 정상 응답\n",
    "            print(\"Url Request Success\")\n",
    "            data = response.read().decode('utf-8')\n",
    "            return data  # 크롤링 결과 반환\n",
    "\n",
    "    except Exception as e:  # url로 요청 중 오류가 발생시 실행\n",
    "        print(e)\n",
    "        print(\"Error for URL : %s\" %url)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    print(\"네이버 검색 프로그램입니다.\")\n",
    "    search_text = input(\">검색할 키워드를 입력하세요.\")\n",
    "\n",
    "    answer = input (\">검색 카테고리를 선택하세요 \\n 1. 네이버 뉴스,\\\n",
    "                    2. 네이버 블로그 3. 네이버 카페\")\n",
    "    sNode = 'news'\n",
    "    if answer=='1' :\n",
    "        sNode = 'news'  # 'news', 'blog', 'cafearticle'\n",
    "    elif answer=='2' :\n",
    "        sNode = 'blog'\n",
    "    elif answer=='3' :\n",
    "        sNode = 'cafearticle'\n",
    "    print(\"*******\",answer)\n",
    "\n",
    "    targetURL = makeURL(sNode, search_text )    #1. URL 만들기\n",
    "    result = requestURL(targetURL)              #2. URL로 크롤링\n",
    "    #print(result)                              #3. 크롤링 결과 출력\n",
    "\n",
    "\n",
    "\n",
    "    #1. 크롤링 전체 결과를 파일에 저장\n",
    "    filename = '%s_naver_%s_all.json' % (search_text, sNode)\n",
    "    with open(filename, 'w', encoding='utf8') as outfile:\n",
    "        outfile.write(result)\n",
    "    print(filename +' 저장 완료' )\n",
    "\n",
    "\n",
    "\n",
    "    #2. 크롤링 결과 중 일부만 파싱할 때\n",
    "    jsonResult = []\n",
    "    if result != None:\n",
    "        jsonSearch = json.loads(result)    # JSON 문자열을  파이썬 딕셔너리로 load\n",
    "\n",
    "        for post in jsonSearch['items']:  # items에 있는 데이터만\n",
    "            title = post['title']\n",
    "            description = post['description']\n",
    "            if sNode=='news' :  #news만 originallink가 있음\n",
    "                org_link = post['originallink']\n",
    "            else :\n",
    "                org_link = \"\"\n",
    "\n",
    "            jsonResult.append({'title': title,\n",
    "                               'description': description,\n",
    "                               'org_link': org_link\n",
    "                                # ,'link': link\n",
    "                              })\n",
    "\n",
    "    retJson = json.dumps(jsonResult,   #파이썬 딕셔너리를 json 문자열로\n",
    "                         indent=4,     #들여쓰기 함\n",
    "                         ensure_ascii=False) #한글 인코딩\n",
    "    print(retJson)\n",
    "\n",
    "\n",
    "\n",
    "    #3. 파싱 결과 일부만 파일에 저장\n",
    "    filename = '%s_naver_%s.json' % (search_text, sNode)\n",
    "    with open(filename, 'w', encoding='utf8') as outfile:\n",
    "        outfile.write(retJson)\n",
    "    print(filename + ' 저장 완료')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kscVjCiAzf-I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 네이버 검색 프로그램입니다.\n",
      "> 검색할 키워드를 입력하세요.후쿠시마\n",
      "> 검색 카테고리를 선택하세요 \n",
      " 1. 네이버 뉴스,                    2. 네이버 블로그 3. 네이버 카페2\n",
      "네이버 블로그로 검색합니다.\n",
      "******* 2\n",
      "Url Request Success\n",
      "> 저장 종류를 선택하세요. \n",
      " 1. 크롤링 전체 결과 파일로 저장                     2. 크롤링 결과 중 일부만 파싱하여 출력 및 파일로 저장2\n",
      "결과 부분 저장.\n",
      "******* 2\n",
      "[\n",
      "    {\n",
      "        \"title\": \"더 데이스는 <b>후쿠시마</b> 원전 사고의 한 면만 담은 아쉬운 작품\",\n",
      "        \"description\": \"그리고 그렇게 안전하다면서 안전한 물을 먹고 산 <b>후쿠시마</b> 어폐류는 왜 수입을 금지하는지... 지금 <b>후쿠시마</b> 원전이 어떤 상태인지 뭔 작업을 하고 있는지 어떤 상태인지 제대로 아는 사람이 몇이나 있을까요?... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"오사카 야키니쿠 맛집 <b>후쿠시마</b>역 시시로\",\n",
      "        \"description\": \"지난 오사카 여행은 처음으로 <b>후쿠시마</b> 지역을 방문했었어요. 우리가 흔히 알고 있는 <b>후쿠시마</b>가 아닌 오사카 우메다에서 한정거장 떨어진 <b>후쿠시마</b>역 입니다. 이곳은 아직 한국 여행자에게 많이 알려지진... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"일본 <b>후쿠시마</b> 핵폐수 투기가 재앙인 이유\",\n",
      "        \"description\": \"일본이 <b>후쿠시마</b> 원자력 발전소에서 나온 핵폐수를 정화 처리한 후 태평양으로 방류하겠다고 합니다. 지난 2011년 <b>후쿠시마</b> 원자력 발전소 사고가 난 이후 축적된 오염수는 약 134만 톤으로, 일본은 오염수를... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"[<b>후쿠시마</b> 오염수] 광우병 사태는 그저 괴담이었나?\",\n",
      "        \"description\": \"△윤석열 대통령은 대국민 설득 없이 독단적으로 <b>후쿠시마</b> 오염수 방류에 찬성하였습니다. △자연스러운 수산물 먹거리 불안을 '과학'으로 맞섭니다. △<b>후쿠시마</b> 오염수 방류에 대한 비판을 괴담, 선동 등... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"<b>후쿠시마</b> 핵오염수\",\n",
      "        \"description\": \"체르노빌도 콘크리트로 덮고 마냥 해결기술이 나올 미래만 기다리는 상황인데 <b>후쿠시마</b>도 같은... 체르노빌 사건과 <b>후쿠시마</b>사건을 보면 핵연료가 자가분열하여 계속온도가 오르면 2800도 이상되어 아래의... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"<b>후쿠시마</b> 오염수 방류 관련 일일브리핑(164일차)\",\n",
      "        \"description\": \"<b>후쿠시마</b> 오염수 방류 관련 일일브리핑(164일차) 2024.02.13 국무조정실 &lt; 1. 인사말씀... 전문가 현지 파견 활동 결과 &gt; □ 지난주 월요일(2.5) 브리핑에서 말씀드렸던 바와 같이, 우리 전문가들이 <b>후쿠시마</b>... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"2차 <b>후쿠시마</b> 방사능 오염수 해양투기와 <b>후쿠시마</b> 수산물 우회... \",\n",
      "        \"description\": \"지난 8월 윤석열 정부의 침묵 속에서 일본 정부와 도쿄전력은 <b>후쿠시마</b> 방사능 오염수 해양투기를 전격... 강조했지만 <b>후쿠시마</b> 원전 사고 당시 일본 정부조차 도쿄전력의 대응 능력과 은폐, 무능 등을 경험한... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"<b>후쿠시마</b> 오염수 방류 발단 일정 정부 대응 등 정리\",\n",
      "        \"description\": \"8월 24일 일본에서 <b>후쿠시마</b> 오염수 방류를 시작하고 3일째다. 상황이 어떻게 진행되고 있는지... 1의 지진)으로 인해 발생한 쓰나미(지진 해일)가 <b>후쿠시마</b> 제1원자력발전소를 덮치고 원전의 전원이 끊겨... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"<b>후쿠시마</b> 오염수 방류 정말 괜찮을까?\",\n",
      "        \"description\": \"https://m.blog.naver.com/adiu2000/223194279497 이재명 대표 : <b>후쿠시마</b> 오염수가 안전하다고?..&quot;그럼 일본 식수로... 하지만 윤석열은 이미 후보 시절에 <b>후쿠시마</b> 오염수는 깨끗하다고 얘기한 바 있다. 난 이재명 대표의... \",\n",
      "        \"org_link\": \"\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"<b>후쿠시마</b> 오염수 방류 후 사건 사고 모음\",\n",
      "        \"description\": \"일본 <b>후쿠시마</b> 방사능 오염수는 2023년 8월 24일부터 방류를 시작했습니다. 이전 <b>후쿠시마</b> 오염수 방류에 대한 논란은 아래에 있습니다. 1. <b>후쿠시마</b> 산 수산물 수입 논란 <b>후쿠시마</b> 방사능 사고로 인해... \",\n",
      "        \"org_link\": \"\"\n",
      "    }\n",
      "]\n",
      "후쿠시마_naver_blog.json 저장 완료\n"
     ]
    }
   ],
   "source": [
    "#lab05\n",
    "# 1. 크롤링 전체 결과를 파일에 저장\n",
    "# 2. 크롤링 결과 중 일부만 ( title, originallink, description )만 파싱하여 출력 및 파일로 저장\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "client_id = \"5BE6Vc9HVCIU7_SKWbY_\"\n",
    "client_secret = \"GQe2e_vSC2\"\n",
    "\n",
    "def makeURL(sNode, search_text ):\n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = \"/%s.json\" % sNode\n",
    "    parameters = \"?query=%s\" % urllib.parse.quote(search_text)\n",
    "    url = base + node + parameters\n",
    "    return url\n",
    "\n",
    "\n",
    "def requestURL(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"X-Naver-Client-Id\",   client_id)  # open api 키를 header에 추가\n",
    "    req.add_header(\"X-Naver-Client-Secret\",  client_secret)  # open api 키를 header에 추가\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)  # 크롤링\n",
    "\n",
    "        if response.status == 200:  # 200이면 정상 응답\n",
    "            print(\"Url Request Success\")\n",
    "            data = response.read().decode('utf-8')\n",
    "            return data  # 크롤링 결과 반환\n",
    "\n",
    "    except Exception as e:  # url로 요청 중 오류가 발생시 실행\n",
    "        print(e)\n",
    "        print(\"Error for URL : %s\" %url)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    print(\"> 네이버 검색 프로그램입니다.\")\n",
    "    search_text = input(\"> 검색할 키워드를 입력하세요.\")\n",
    "\n",
    "    answer = input(\"> 검색 카테고리를 선택하세요 \\n 1. 네이버 뉴스,\\\n",
    "                    2. 네이버 블로그 3. 네이버 카페\")\n",
    "    \n",
    "    sNode = 'news'\n",
    "    if answer=='1' :\n",
    "        print(\"네이버 뉴스로 검색합니다.\")\n",
    "        sNode = 'news'  # 'news', 'blog', 'cafearticle'\n",
    "    elif answer=='2' :\n",
    "        print(\"네이버 블로그로 검색합니다.\")\n",
    "        sNode = 'blog'\n",
    "    elif answer=='3' :\n",
    "        print(\"네이버 카페로 검색합니다.\")\n",
    "        sNode = 'cafearticle'\n",
    "    print(\"*******\",answer)\n",
    "\n",
    "    targetURL = makeURL(sNode, search_text )    #1. URL 만들기\n",
    "    result = requestURL(targetURL)              #2. URL로 크롤링\n",
    "    #print(result)                              #3. 크롤링 결과 출력\n",
    "\n",
    "    check_answer = input (\"> 저장 종류를 선택하세요. \\n 1. 크롤링 전체 결과 파일로 저장 \\\n",
    "                    2. 크롤링 결과 중 일부만 파싱하여 출력 및 파일로 저장\")\n",
    "    \n",
    "    cNode = 'entire'\n",
    "    if check_answer=='1' :\n",
    "        print(\"결과 전체 저장.\")\n",
    "        cNode = 'entire'  \n",
    "    elif check_answer =='2' :\n",
    "        print(\"결과 부분 저장.\")\n",
    "        cNode = 'part'\n",
    "    print(\"*******\",check_answer)\n",
    "\n",
    "    if cNode == 'entrie':\n",
    "        #1. 크롤링 전체 결과를 파일에 저장\n",
    "        filename = '%s_naver_%s_all.json' % (search_text, sNode)\n",
    "        with open(filename, 'w', encoding='utf8') as outfile:\n",
    "            outfile.write(result)\n",
    "        print(filename +' 저장 완료' )\n",
    "        \n",
    "    elif cNode == 'part':\n",
    "        #2. 크롤링 결과 중 일부만 파싱할 때\n",
    "        jsonResult = []\n",
    "        if result != None:\n",
    "            jsonSearch = json.loads(result)    # JSON 문자열을  파이썬 딕셔너리로 load\n",
    "\n",
    "            for post in jsonSearch['items']:  # items에 있는 데이터만\n",
    "                title = post['title']\n",
    "                description = post['description']\n",
    "                if sNode=='news' :  #news만 originallink가 있음\n",
    "                    org_link = post['originallink']\n",
    "                else :\n",
    "                    org_link = \"\"\n",
    "\n",
    "                jsonResult.append({'title': title,\n",
    "                                   'description': description,\n",
    "                                   'org_link': org_link\n",
    "                                    # ,'link': link\n",
    "                                  })\n",
    "\n",
    "        retJson = json.dumps(jsonResult,   #파이썬 딕셔너리를 json 문자열로\n",
    "                             indent=4,     #들여쓰기 함\n",
    "                             ensure_ascii=False) #한글 인코딩\n",
    "        print(retJson)\n",
    "\n",
    "\n",
    "\n",
    "        #3. 파싱 결과 일부만 파일에 저장\n",
    "        filename = '%s_naver_%s.json' % (search_text, sNode)\n",
    "        with open(filename, 'w', encoding='utf8') as outfile:\n",
    "            outfile.write(retJson)\n",
    "        print(filename + ' 저장 완료')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\" :\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
